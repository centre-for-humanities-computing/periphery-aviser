{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.4.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "Collecting numpy\n",
      "  Downloading numpy-2.2.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Collecting seaborn\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.6.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting filelock (from datasets)\n",
      "  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-19.0.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/ucloud/.local/lib/python3.12/site-packages (from datasets) (2.32.3)\n",
      "Collecting tqdm>=4.66.3 (from datasets)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.11.14-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting huggingface-hub>=0.24.0 (from datasets)\n",
      "  Downloading huggingface_hub-0.29.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging in /usr/lib/python3/dist-packages (from datasets) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ucloud/.local/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.56.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (101 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.2 kB)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Downloading pillow-11.1.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.1 kB)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/ucloud/.local/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2025.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.15.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ucloud/.local/lib/python3.12/site-packages (from aiohttp->datasets) (25.3.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Downloading frozenlist-1.5.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->datasets)\n",
      "  Downloading propcache-0.3.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.18.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (69 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ucloud/.local/lib/python3.12/site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ucloud/.local/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ucloud/.local/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ucloud/.local/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ucloud/.local/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Downloading datasets-3.4.1-py3-none-any.whl (487 kB)\n",
      "Downloading matplotlib-3.10.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.2.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.1/16.1 MB\u001b[0m \u001b[31m57.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Downloading scikit_learn-1.6.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m56.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.3.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (323 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading fonttools-4.56.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "Downloading aiohttp-3.11.14-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.29.3-py3-none-any.whl (468 kB)\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading kiwisolver-1.4.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Downloading pillow-11.1.0-cp312-cp312-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-19.0.1-cp312-cp312-manylinux_2_28_x86_64.whl (42.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m100.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.1-py2.py3-none-any.whl (507 kB)\n",
      "Downloading scipy-1.15.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.3/37.3 MB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Downloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Downloading xxhash-3.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading frozenlist-1.5.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (283 kB)\n",
      "Downloading multidict-6.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (132 kB)\n",
      "Downloading propcache-0.3.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (243 kB)\n",
      "Downloading yarl-1.18.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (336 kB)\n",
      "Installing collected packages: pytz, xxhash, tzdata, tqdm, threadpoolctl, pyarrow, propcache, pillow, numpy, multidict, kiwisolver, joblib, fsspec, frozenlist, fonttools, filelock, dill, cycler, aiohappyeyeballs, yarl, scipy, pandas, multiprocess, huggingface-hub, contourpy, aiosignal, scikit-learn, matplotlib, aiohttp, seaborn, datasets\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.11.14 aiosignal-1.3.2 contourpy-1.3.1 cycler-0.12.1 datasets-3.4.1 dill-0.3.8 filelock-3.18.0 fonttools-4.56.0 frozenlist-1.5.0 fsspec-2024.12.0 huggingface-hub-0.29.3 joblib-1.4.2 kiwisolver-1.4.8 matplotlib-3.10.1 multidict-6.2.0 multiprocess-0.70.16 numpy-2.2.4 pandas-2.2.3 pillow-11.1.0 propcache-0.3.0 pyarrow-19.0.1 pytz-2025.1 scikit-learn-1.6.1 scipy-1.15.2 seaborn-0.13.2 threadpoolctl-3.6.0 tqdm-4.67.1 tzdata-2025.2 xxhash-3.5.0 yarl-1.18.3\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets matplotlib pandas numpy seaborn scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset, DatasetDict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "newspapers = ['lf_all', 'aal_all', 'od_all_clean', 'thi_all', 'vib_all']\n",
    "path_root = '../../../DATA/NEWSPAPERS/article_embs/embeddings_e5/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: finding the best classifier, predicting unlabeled data for manual evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['article_id', 'date', 'embedding', 'n_chunks_orig', 'clean_category',\n",
       "       'nøgle', 'text', 'category', 'article_length', 'characters'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = Dataset.load_from_disk(f'{path_root}{newspapers[0]}')\n",
    "df = ds.to_pandas()\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85489, 10)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85092, 11)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the shape of each embedding\n",
    "df['embedding_shape'] = df['embedding'].apply(lambda x: np.array(x).shape)\n",
    "expected_dim = df['embedding_shape'].max()[0]\n",
    "df = df[df['embedding'].apply(lambda x: np.array(x).shape == (expected_dim,))].copy()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "clean_category\n",
       "Bekjendtgjørelser    35817\n",
       "Jndenlandsk          25184\n",
       "Udenlandsk           24091\n",
       "Name: clean_category, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('clean_category')['clean_category'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1427/1606508774.py:5: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_balanced = df.groupby('clean_category', group_keys=False).apply(\n"
     ]
    }
   ],
   "source": [
    "# Define the number of samples per class (adjust based on dataset size)\n",
    "n_samples_per_class = 24000  # Change as needed\n",
    "\n",
    "# Create a balanced dataset by sampling an equal number of instances per class\n",
    "df_balanced = df.groupby('clean_category', group_keys=False).apply(\n",
    "    lambda x: x.sample(n=min(len(x), n_samples_per_class), random_state=42)\n",
    ")\n",
    "\n",
    "# Split the balanced data into train and test sets with stratification\n",
    "train_df, test_df = train_test_split(\n",
    "    df_balanced, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=df_balanced['clean_category']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train classifier on embeddings\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Bekjendtgjørelser       0.91      0.94      0.93      4800\n",
      "      Jndenlandsk       0.84      0.85      0.84      4800\n",
      "       Udenlandsk       0.91      0.88      0.89      4800\n",
      "\n",
      "         accuracy                           0.89     14400\n",
      "        macro avg       0.89      0.89      0.89     14400\n",
      "     weighted avg       0.89      0.89      0.89     14400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare training and test features/labels\n",
    "X_train = np.vstack(train_df['embedding'].values)\n",
    "y_train = train_df['clean_category'].values\n",
    "\n",
    "X_test = np.vstack(test_df['embedding'].values)\n",
    "y_test = test_df['clean_category'].values\n",
    "\n",
    "# Instantiate the Logistic Regression classifier\n",
    "clf_embs = LogisticRegression(max_iter=1000, solver='liblinear', random_state=42)\n",
    "\n",
    "# Train the classifier on the labeled training data\n",
    "print(f'Train classifier on embeddings')\n",
    "clf_embs.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on the test set\n",
    "predictions = clf_embs.predict(X_test)\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train classifier on embeddings\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Bekjendtgjørelser       0.92      0.95      0.94      4800\n",
      "      Jndenlandsk       0.85      0.85      0.85      4800\n",
      "       Udenlandsk       0.90      0.87      0.88      4800\n",
      "\n",
      "         accuracy                           0.89     14400\n",
      "        macro avg       0.89      0.89      0.89     14400\n",
      "     weighted avg       0.89      0.89      0.89     14400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare training and test features/labels\n",
    "X_train = np.vstack(train_df['embedding'].values)\n",
    "y_train = train_df['clean_category'].values\n",
    "\n",
    "X_test = np.vstack(test_df['embedding'].values)\n",
    "y_test = test_df['clean_category'].values\n",
    "\n",
    "# Instantiate the Logistic Regression classifier\n",
    "clf_embs = LogisticRegression(max_iter=1000, solver='liblinear', random_state=42)\n",
    "\n",
    "# Train the classifier on the labeled training data\n",
    "print(f'Train classifier on embeddings')\n",
    "clf_embs.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on the test set\n",
    "predictions = clf_embs.predict(X_test)\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train classifier on TF-IDF features\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Bekjendtgjørelser       0.94      0.96      0.95      4800\n",
      "      Jndenlandsk       0.88      0.86      0.87      4800\n",
      "       Udenlandsk       0.90      0.91      0.91      4800\n",
      "\n",
      "         accuracy                           0.91     14400\n",
      "        macro avg       0.91      0.91      0.91     14400\n",
      "     weighted avg       0.91      0.91      0.91     14400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=5000)  # Adjust max_features as needed\n",
    "\n",
    "# Fit on training data and transform both train and test sets\n",
    "X_train = vectorizer.fit_transform(train_df['text'])\n",
    "X_test = vectorizer.transform(test_df['text'])\n",
    "\n",
    "# Prepare labels\n",
    "y_train = train_df['clean_category'].values\n",
    "y_test = test_df['clean_category'].values\n",
    "\n",
    "# Instantiate the Logistic Regression classifier\n",
    "clf_tfidf = LogisticRegression(max_iter=1000, solver='liblinear', random_state=42)\n",
    "\n",
    "# Train the classifier on the TF-IDF features\n",
    "print(f'Train classifier on TF-IDF features')\n",
    "clf_tfidf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on the test set\n",
    "predictions = clf_tfidf.predict(X_test)\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get unlabeled articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(599929, 11)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create final_df with all unlabeled articles\n",
    "dfs = []\n",
    "\n",
    "for i in range(1,5):\n",
    "    ds = Dataset.load_from_disk(f'{path_root}{newspapers[i]}')\n",
    "    df_np = ds.to_pandas()\n",
    "    dfs.append(df_np)\n",
    "\n",
    "final_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Check the shape of each embedding\n",
    "final_df['embedding_shape'] = final_df['embedding'].apply(lambda x: np.array(x).shape)\n",
    "expected_dim = final_df['embedding_shape'].max()[0]\n",
    "final_df = final_df[final_df['embedding'].apply(lambda x: np.array(x).shape == (expected_dim,))].copy()\n",
    "\n",
    "# Add column with name newspaper\n",
    "final_df['newspaper'] = final_df['article_id'].str.extract(r'^(.*?)_')\n",
    "\n",
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1449/2593171623.py:1: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_sampled = final_df.groupby('newspaper', group_keys=False).apply(lambda x: x.sample(n=200, random_state=42))\n"
     ]
    }
   ],
   "source": [
    "# Create a balanced sample for testing and manually evaluating\n",
    "df_sampled = final_df.groupby('newspaper', group_keys=False).apply(lambda x: x.sample(n=200, random_state=42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the new unlabeled data using the trained TF-IDF vectorizer\n",
    "X_test_tfidf = vectorizer.transform(df_sampled['text'])\n",
    "\n",
    "# Predict categories for the new data based on tf_idf\n",
    "df_sampled['predicted_category_tf_idf'] = clf_tfidf.predict(X_test_tfidf)\n",
    "\n",
    "X_test_embs = np.vstack(df_sampled['embedding'].values)\n",
    "\n",
    "# Predict categories for the new data based on embeddings\n",
    "df_sampled['predicted_category_embs'] = clf_embs.predict(X_test_embs)\n",
    "\n",
    "# Save results\n",
    "df_sampled[['article_id', 'date', 'predicted_category_tf_idf', 'predicted_category_embs', 'category', 'text']].to_csv('../results/predicted_sample_tfidf_embeddings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disagree = df_sampled[df_sampled['predicted_category_embs'] != df_sampled['predicted_category_tf_idf']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(117, 14)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_disagree.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: using the best classifier to predict all unlabeled articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(685021, 12)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create final_df with all unlabeled articles\n",
    "dfs = []\n",
    "\n",
    "for i in range(0,5):\n",
    "    ds = Dataset.load_from_disk(f'{path_root}{newspapers[i]}')\n",
    "    df_np = ds.to_pandas()\n",
    "    dfs.append(df_np)\n",
    "\n",
    "final_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Check the shape of each embedding\n",
    "final_df['embedding_shape'] = final_df['embedding'].apply(lambda x: np.array(x).shape)\n",
    "expected_dim = final_df['embedding_shape'].max()[0]\n",
    "final_df = final_df[final_df['embedding'].apply(lambda x: np.array(x).shape == (expected_dim,))].copy()\n",
    "\n",
    "# Add column with name newspaper\n",
    "final_df['newspaper'] = final_df['article_id'].str.extract(r'^(.*?)_')\n",
    "\n",
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>date</th>\n",
       "      <th>embedding</th>\n",
       "      <th>n_chunks_orig</th>\n",
       "      <th>clean_category</th>\n",
       "      <th>nøgle</th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>article_length</th>\n",
       "      <th>characters</th>\n",
       "      <th>embedding_shape</th>\n",
       "      <th>newspaper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lol_000001</td>\n",
       "      <td>1809-03-07</td>\n",
       "      <td>[0.015812713303603232, 0.0059686582535505295, ...</td>\n",
       "      <td>4</td>\n",
       "      <td>Jndenlandsk</td>\n",
       "      <td>1809-03-07_5</td>\n",
       "      <td>Jndenlandsk. Helsingøer den 26de Februar. J Fo...</td>\n",
       "      <td>Jndenlandsk</td>\n",
       "      <td>298</td>\n",
       "      <td>1592</td>\n",
       "      <td>(1024,)</td>\n",
       "      <td>lol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lol_000002</td>\n",
       "      <td>1809-03-07</td>\n",
       "      <td>[0.019437216222286224, 0.013088210485875607, -...</td>\n",
       "      <td>1</td>\n",
       "      <td>Jndenlandsk</td>\n",
       "      <td>1809-03-07_8</td>\n",
       "      <td>Kjøbenhavn den 27 Februarii. Kornpriserne i Kb...</td>\n",
       "      <td>Jndenlandsk</td>\n",
       "      <td>81</td>\n",
       "      <td>513</td>\n",
       "      <td>(1024,)</td>\n",
       "      <td>lol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lol_000003</td>\n",
       "      <td>1809-03-07</td>\n",
       "      <td>[0.006957924459129572, 0.006066289730370045, -...</td>\n",
       "      <td>1</td>\n",
       "      <td>Jndenlandsk</td>\n",
       "      <td>1809-03-07_9</td>\n",
       "      <td>De danske Officerer af Linieskibet Prinds Chri...</td>\n",
       "      <td>Jndenlandsk</td>\n",
       "      <td>31</td>\n",
       "      <td>205</td>\n",
       "      <td>(1024,)</td>\n",
       "      <td>lol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lol_000004</td>\n",
       "      <td>1809-03-07</td>\n",
       "      <td>[0.044964227825403214, 0.006043573841452599, -...</td>\n",
       "      <td>1</td>\n",
       "      <td>Jndenlandsk</td>\n",
       "      <td>1809-03-07_10</td>\n",
       "      <td>Caffe koster nu paa det Vestindiske Compagnie ...</td>\n",
       "      <td>Jndenlandsk</td>\n",
       "      <td>48</td>\n",
       "      <td>257</td>\n",
       "      <td>(1024,)</td>\n",
       "      <td>lol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lol_000005</td>\n",
       "      <td>1809-03-07</td>\n",
       "      <td>[0.031310804188251495, 0.01853332109749317, -0...</td>\n",
       "      <td>1</td>\n",
       "      <td>Jndenlandsk</td>\n",
       "      <td>1809-03-07_11</td>\n",
       "      <td>For kort Tid siden bleve de, der boe ved Veste...</td>\n",
       "      <td>Jndenlandsk</td>\n",
       "      <td>80</td>\n",
       "      <td>445</td>\n",
       "      <td>(1024,)</td>\n",
       "      <td>lol</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id        date                                          embedding  \\\n",
       "0  lol_000001  1809-03-07  [0.015812713303603232, 0.0059686582535505295, ...   \n",
       "1  lol_000002  1809-03-07  [0.019437216222286224, 0.013088210485875607, -...   \n",
       "2  lol_000003  1809-03-07  [0.006957924459129572, 0.006066289730370045, -...   \n",
       "3  lol_000004  1809-03-07  [0.044964227825403214, 0.006043573841452599, -...   \n",
       "4  lol_000005  1809-03-07  [0.031310804188251495, 0.01853332109749317, -0...   \n",
       "\n",
       "   n_chunks_orig clean_category          nøgle  \\\n",
       "0              4    Jndenlandsk   1809-03-07_5   \n",
       "1              1    Jndenlandsk   1809-03-07_8   \n",
       "2              1    Jndenlandsk   1809-03-07_9   \n",
       "3              1    Jndenlandsk  1809-03-07_10   \n",
       "4              1    Jndenlandsk  1809-03-07_11   \n",
       "\n",
       "                                                text     category  \\\n",
       "0  Jndenlandsk. Helsingøer den 26de Februar. J Fo...  Jndenlandsk   \n",
       "1  Kjøbenhavn den 27 Februarii. Kornpriserne i Kb...  Jndenlandsk   \n",
       "2  De danske Officerer af Linieskibet Prinds Chri...  Jndenlandsk   \n",
       "3  Caffe koster nu paa det Vestindiske Compagnie ...  Jndenlandsk   \n",
       "4  For kort Tid siden bleve de, der boe ved Veste...  Jndenlandsk   \n",
       "\n",
       "   article_length  characters embedding_shape newspaper  \n",
       "0             298        1592         (1024,)       lol  \n",
       "1              81         513         (1024,)       lol  \n",
       "2              31         205         (1024,)       lol  \n",
       "3              48         257         (1024,)       lol  \n",
       "4              80         445         (1024,)       lol  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "clean_category\n",
       "-1                   599929\n",
       "Bekjendtgjørelser     35817\n",
       "Jndenlandsk           25184\n",
       "Udenlandsk            24091\n",
       "Name: clean_category, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.groupby('clean_category')['clean_category'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>date</th>\n",
       "      <th>predicted_category_tf_idf</th>\n",
       "      <th>evaluation_tf_idf</th>\n",
       "      <th>predicted_category_embs</th>\n",
       "      <th>evaluation_embs</th>\n",
       "      <th>category</th>\n",
       "      <th>true_label</th>\n",
       "      <th>bog_teater</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>55097</th>\n",
       "      <td>aal_055098</td>\n",
       "      <td>1826-08-17</td>\n",
       "      <td>Jndenlandsk</td>\n",
       "      <td>t</td>\n",
       "      <td>Jndenlandsk</td>\n",
       "      <td>t</td>\n",
       "      <td>Kjøbenhavn.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>J Nærheden heraf er opkommet en Jordbrand, som...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142457</th>\n",
       "      <td>aal_142458</td>\n",
       "      <td>1840-12-05</td>\n",
       "      <td>Jndenlandsk</td>\n",
       "      <td>t</td>\n",
       "      <td>Bekjendtgjørelser</td>\n",
       "      <td>t</td>\n",
       "      <td>Bekjendtgjørelser</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ved Auctionen, Torsdagen den 10de dennes, i St...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70491</th>\n",
       "      <td>aal_070492</td>\n",
       "      <td>1829-03-04</td>\n",
       "      <td>Udenlandsk</td>\n",
       "      <td>t</td>\n",
       "      <td>Udenlandsk</td>\n",
       "      <td>t</td>\n",
       "      <td>Blandinger</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>En Kok i Paris har taget Livet af sig, fordi m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146810</th>\n",
       "      <td>aal_146811</td>\n",
       "      <td>1841-08-10</td>\n",
       "      <td>Udenlandsk</td>\n",
       "      <td>t</td>\n",
       "      <td>Udenlandsk</td>\n",
       "      <td>t</td>\n",
       "      <td>Nyeste Post-Efterretninger</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Det hollandske Blad, Tolk der Vryheid, med hvi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155768</th>\n",
       "      <td>aal_155769</td>\n",
       "      <td>1842-12-07</td>\n",
       "      <td>Jndenlandsk</td>\n",
       "      <td>t</td>\n",
       "      <td>Jndenlandsk</td>\n",
       "      <td>t</td>\n",
       "      <td>Fædrelandet</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>J Sagen: Postholder Prahl, for, imod ProviHarm...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        article_id        date predicted_category_tf_idf evaluation_tf_idf  \\\n",
       "55097   aal_055098  1826-08-17               Jndenlandsk                 t   \n",
       "142457  aal_142458  1840-12-05               Jndenlandsk                 t   \n",
       "70491   aal_070492  1829-03-04                Udenlandsk                 t   \n",
       "146810  aal_146811  1841-08-10                Udenlandsk                 t   \n",
       "155768  aal_155769  1842-12-07               Jndenlandsk                 t   \n",
       "\n",
       "       predicted_category_embs evaluation_embs                    category  \\\n",
       "55097              Jndenlandsk               t                 Kjøbenhavn.   \n",
       "142457       Bekjendtgjørelser               t           Bekjendtgjørelser   \n",
       "70491               Udenlandsk               t                  Blandinger   \n",
       "146810              Udenlandsk               t  Nyeste Post-Efterretninger   \n",
       "155768             Jndenlandsk               t                 Fædrelandet   \n",
       "\n",
       "       true_label bog_teater  \\\n",
       "55097         NaN        NaN   \n",
       "142457        NaN        NaN   \n",
       "70491         NaN        NaN   \n",
       "146810        NaN        NaN   \n",
       "155768        NaN        NaN   \n",
       "\n",
       "                                                     text  \n",
       "55097   J Nærheden heraf er opkommet en Jordbrand, som...  \n",
       "142457  Ved Auctionen, Torsdagen den 10de dennes, i St...  \n",
       "70491   En Kok i Paris har taget Livet af sig, fordi m...  \n",
       "146810  Det hollandske Blad, Tolk der Vryheid, med hvi...  \n",
       "155768  J Sagen: Postholder Prahl, for, imod ProviHarm...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold_set = pd.read_csv('../results/predicted_sample_tfidf_embeddings(1).csv', index_col=0, sep=';')\n",
    "gold_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_269/3149177884.py:1: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  gold_set = gold_set.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n"
     ]
    }
   ],
   "source": [
    "gold_set = gold_set.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "\n",
    "gold_set[\"true_label\"] = gold_set.apply(\n",
    "    lambda row: row[\"predicted_category_tf_idf\"] if row[\"evaluation_tf_idf\"] == \"t\" else\n",
    "                (row[\"predicted_category_embs\"] if row[\"evaluation_embs\"] == \"t\" else row[\"true_label\"]),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge final_df with df_gold_set on 'article_id' to get updated values\n",
    "final_df = final_df.merge(gold_set[['article_id', 'true_label']], on='article_id', how='left')\n",
    "\n",
    "# Overwrite 'clean_category' with 'true_label' where available\n",
    "final_df['clean_category'] = final_df['true_label'].fillna(final_df['clean_category'])\n",
    "\n",
    "# Drop the temporary 'true_label' column\n",
    "final_df.drop(columns=['true_label'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['label_type'] = final_df['clean_category'].apply(lambda x: 'predicted' if x == -1 else 'gold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85882, 13)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df = final_df[final_df['label_type'] == 'gold']\n",
    "training_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_269/3735447762.py:5: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  lol_balanced = lol_df.groupby('clean_category', group_keys=False).apply(\n"
     ]
    }
   ],
   "source": [
    "# Identify the subset where 'newspaper' == 'lol'\n",
    "lol_df = training_df[training_df['newspaper'] == 'lol']\n",
    "\n",
    "# Sample 200 random examples for each value of 'clean_category'\n",
    "lol_balanced = lol_df.groupby('clean_category', group_keys=False).apply(\n",
    "    lambda x: x.sample(n=400, replace=True) if len(x) >= 400 else x\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# Keep the other newspapers as they are\n",
    "other_newspapers_df = training_df[training_df['newspaper'] != 'lol']\n",
    "\n",
    "# Combine the balanced 'lol' subset with the other newspapers\n",
    "filtered_df = pd.concat([lol_balanced, other_newspapers_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train final classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the balanced data into train and test sets with stratification\n",
    "train_df, test_df = train_test_split(\n",
    "    filtered_df, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=filtered_df['clean_category']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train classifier on embeddings\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Bekjendtgjørelser       0.86      0.88      0.87       136\n",
      "      Jndenlandsk       0.75      0.76      0.75       136\n",
      "       Udenlandsk       0.88      0.85      0.86       126\n",
      "\n",
      "         accuracy                           0.83       398\n",
      "        macro avg       0.83      0.83      0.83       398\n",
      "     weighted avg       0.83      0.83      0.83       398\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare training and test features/labels\n",
    "X_train = np.vstack(train_df['embedding'].values)\n",
    "y_train = train_df['clean_category'].values\n",
    "\n",
    "X_test = np.vstack(test_df['embedding'].values)\n",
    "y_test = test_df['clean_category'].values\n",
    "\n",
    "# Instantiate the Logistic Regression classifier\n",
    "clf_embs = LogisticRegression(max_iter=1000, solver='liblinear', random_state=42)\n",
    "\n",
    "# Train the classifier on the labeled training data\n",
    "print(f'Train classifier on embeddings')\n",
    "clf_embs.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on the test set\n",
    "predictions = clf_embs.predict(X_test)\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(599139, 13)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df = final_df[final_df['label_type'] == 'predicted']\n",
    "pred_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_269/2012251868.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  pred_df['predicted_category_embs'] = clf_embs.predict(X_test_embs)\n"
     ]
    }
   ],
   "source": [
    "X_test_embs = np.vstack(pred_df['embedding'].values)\n",
    "\n",
    "# Predict categories for the unlabeled articles based on embeddings\n",
    "pred_df['predicted_category_embs'] = clf_embs.predict(X_test_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge final_df with pred_df on 'article_id' to get updated values\n",
    "final_df = final_df.merge(pred_df[['article_id', 'predicted_category_embs']], on='article_id', how='left')\n",
    "\n",
    "# Overwrite 'clean_category' with 'predicted_category_embs' where available\n",
    "final_df['clean_category'] = final_df['predicted_category_embs'].fillna(final_df['clean_category'])\n",
    "\n",
    "# Drop the temporary 'true_label' column\n",
    "final_df.drop(columns=['predicted_category_embs'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(685021, 13)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save articles and their category as parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_parquet(\"../../../DATA/NEWSPAPERS/final_df_e5.parquet\", engine=\"pyarrow\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "HfHubHTTPError",
     "evalue": "401 Client Error: Unauthorized for url: https://huggingface.co/api/repos/create (Request ID: Root=1-67e12a5d-0752c99e1acd1f3e2dae440f;78524486-b840-42f6-9082-e49be6cfbc3d)\n\nInvalid username or password.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:409\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m409\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/requests/models.py:1024\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1023\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1024\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPError\u001b[39m: 401 Client Error: Unauthorized for url: https://huggingface.co/api/repos/create",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mHfHubHTTPError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m dataset = Dataset.from_pandas(final_df, preserve_index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Push to Hugging Face as a dataset\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpush_to_hub\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mawlassche/periphery-aviser-e5\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprivate\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/datasets/arrow_dataset.py:5533\u001b[39m, in \u001b[36mDataset.push_to_hub\u001b[39m\u001b[34m(self, repo_id, config_name, set_default, split, data_dir, commit_message, commit_description, private, token, revision, create_pr, max_shard_size, num_shards, embed_external_files)\u001b[39m\n\u001b[32m   5529\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSplit name should match \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_split_re\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m but got \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   5531\u001b[39m api = HfApi(endpoint=config.HF_ENDPOINT, token=token)\n\u001b[32m-> \u001b[39m\u001b[32m5533\u001b[39m repo_url = \u001b[43mapi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_repo\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5534\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5535\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5536\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdataset\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   5537\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprivate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprivate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5538\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   5539\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5540\u001b[39m repo_id = repo_url.repo_id\n\u001b[32m   5542\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m revision \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m revision.startswith(\u001b[33m\"\u001b[39m\u001b[33mrefs/pr/\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   5543\u001b[39m     \u001b[38;5;66;03m# We do not call create_branch for a PR reference: 400 Bad Request\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/huggingface_hub/hf_api.py:3511\u001b[39m, in \u001b[36mHfApi.create_repo\u001b[39m\u001b[34m(self, repo_id, token, private, repo_type, exist_ok, resource_group_id, space_sdk, space_hardware, space_storage, space_sleep_time, space_secrets, space_variables)\u001b[39m\n\u001b[32m   3508\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   3510\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3511\u001b[39m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3512\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m   3513\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exist_ok \u001b[38;5;129;01mand\u001b[39;00m err.response.status_code == \u001b[32m409\u001b[39m:\n\u001b[32m   3514\u001b[39m         \u001b[38;5;66;03m# Repo already exists and `exist_ok=True`\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:481\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    477\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    479\u001b[39m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[32m    480\u001b[39m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m481\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, \u001b[38;5;28mstr\u001b[39m(e), response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mHfHubHTTPError\u001b[39m: 401 Client Error: Unauthorized for url: https://huggingface.co/api/repos/create (Request ID: Root=1-67e12a5d-0752c99e1acd1f3e2dae440f;78524486-b840-42f6-9082-e49be6cfbc3d)\n\nInvalid username or password."
     ]
    }
   ],
   "source": [
    "# Convert DataFrame to Dataset\n",
    "dataset = Dataset.from_pandas(final_df, preserve_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "215c2b72ca4649fa8e4c21dae07bc585",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a3f1c601e364ca5a8d6b0bd056b2338",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c259aed665245878c17bfada758341a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/53 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "995a4e0b5c9f421297b7a35d5c9067f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/53 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a49b9be32b3c4853a834fed177f3c708",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/53 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bce65d6bc6334e8c9a08a7efcd1d9b91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/53 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6228700c34a409ca2f602adefa7734d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/53 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd6cf37d0f5242059a1a5261c4b9b372",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/53 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b5b6bcf5eb8492fb1cfee73f8273d08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/53 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f02ac291a45486d81a8052b258bfafa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/53 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2837458101d74b37a0934fe95226dee5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/53 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d256dc07adb4561b17a81195bba8f84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/53 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa6d12121b564f0b86dca2faa07a11f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/53 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b8979b540cd43f7b071fff196eae92e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/53 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a586c842ca544a8ac067f1341d1f96d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/53 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/awlassche/periphery-aviser-e5/commit/6e8115f1251e777d3d9bed3967834ec3171aed4c', commit_message='Upload dataset', commit_description='', oid='6e8115f1251e777d3d9bed3967834ec3171aed4c', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/awlassche/periphery-aviser-e5', endpoint='https://huggingface.co', repo_type='dataset', repo_id='awlassche/periphery-aviser-e5'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Push to Hugging Face as a dataset\n",
    "dataset.push_to_hub(\"awlassche/periphery-aviser-e5\", private=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
