{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets in /home/ucloud/.local/lib/python3.12/site-packages (3.6.0)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: pandas in /home/ucloud/.local/lib/python3.12/site-packages (2.3.0)\n",
      "Requirement already satisfied: numpy in /home/ucloud/.local/lib/python3.12/site-packages (2.3.1)\n",
      "Collecting seaborn\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (17 kB)\n",
      "Requirement already satisfied: filelock in /home/ucloud/.local/lib/python3.12/site-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/ucloud/.local/lib/python3.12/site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/ucloud/.local/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/ucloud/.local/lib/python3.12/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/ucloud/.local/lib/python3.12/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /home/ucloud/.local/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/ucloud/.local/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /home/ucloud/.local/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /home/ucloud/.local/lib/python3.12/site-packages (from datasets) (0.33.0)\n",
      "Requirement already satisfied: packaging in /usr/lib/python3/dist-packages (from datasets) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ucloud/.local/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.58.4-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (106 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.2 kB)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Downloading pillow-11.2.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (8.9 kB)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/ucloud/.local/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ucloud/.local/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ucloud/.local/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Collecting scipy>=1.8.0 (from scikit-learn)\n",
      "  Downloading scipy-1.16.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (61 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/ucloud/.local/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.13)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ucloud/.local/lib/python3.12/site-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /home/ucloud/.local/lib/python3.12/site-packages (from huggingface-hub>=0.24.0->datasets) (1.1.5)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ucloud/.local/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ucloud/.local/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ucloud/.local/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ucloud/.local/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/ucloud/.local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ucloud/.local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ucloud/.local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ucloud/.local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ucloud/.local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.5.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/ucloud/.local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/ucloud/.local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Downloading matplotlib-3.10.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Downloading scikit_learn-1.7.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.5/12.5 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (323 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.58.4-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Downloading kiwisolver-1.4.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pillow-11.2.1-cp312-cp312-manylinux_2_28_x86_64.whl (4.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.16.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.1/35.1 MB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, pillow, kiwisolver, joblib, fonttools, cycler, contourpy, scikit-learn, matplotlib, seaborn\n",
      "Successfully installed contourpy-1.3.2 cycler-0.12.1 fonttools-4.58.4 joblib-1.5.1 kiwisolver-1.4.8 matplotlib-3.10.3 pillow-11.2.1 scikit-learn-1.7.0 scipy-1.16.0 seaborn-0.13.2 threadpoolctl-3.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets matplotlib pandas numpy seaborn scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset, DatasetDict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "newspapers = ['lf_all', 'aal_all', 'od_all_clean', 'thi_all', 'vib_all']\n",
    "newspaper_aar = ['aar_all']\n",
    "newspaper_extra = ['aar_ext', 'rib_all', 'sla_all']\n",
    "path_root = '../../../ROOT/DATA/NEWSPAPERS/article_embs/embeddings_e5/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: finding the best classifier, predicting unlabeled data for manual evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['article_id', 'date', 'embedding', 'n_chunks_orig', 'clean_category',\n",
       "       'nøgle', 'text', 'category', 'article_length', 'characters'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = Dataset.load_from_disk(f'{path_root}{newspapers[0]}')\n",
    "df = ds.to_pandas()\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85489, 10)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85092, 11)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the shape of each embedding\n",
    "df['embedding_shape'] = df['embedding'].apply(lambda x: np.array(x).shape)\n",
    "expected_dim = df['embedding_shape'].max()[0]\n",
    "df = df[df['embedding'].apply(lambda x: np.array(x).shape == (expected_dim,))].copy()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "clean_category\n",
       "Bekjendtgjørelser    35817\n",
       "Jndenlandsk          25184\n",
       "Udenlandsk           24091\n",
       "Name: clean_category, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('clean_category')['clean_category'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1427/1606508774.py:5: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_balanced = df.groupby('clean_category', group_keys=False).apply(\n"
     ]
    }
   ],
   "source": [
    "# Define the number of samples per class (adjust based on dataset size)\n",
    "n_samples_per_class = 24000  # Change as needed\n",
    "\n",
    "# Create a balanced dataset by sampling an equal number of instances per class\n",
    "df_balanced = df.groupby('clean_category', group_keys=False).apply(\n",
    "    lambda x: x.sample(n=min(len(x), n_samples_per_class), random_state=42)\n",
    ")\n",
    "\n",
    "# Split the balanced data into train and test sets with stratification\n",
    "train_df, test_df = train_test_split(\n",
    "    df_balanced, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=df_balanced['clean_category']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train classifier on embeddings\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Bekjendtgjørelser       0.91      0.94      0.93      4800\n",
      "      Jndenlandsk       0.84      0.85      0.84      4800\n",
      "       Udenlandsk       0.91      0.88      0.89      4800\n",
      "\n",
      "         accuracy                           0.89     14400\n",
      "        macro avg       0.89      0.89      0.89     14400\n",
      "     weighted avg       0.89      0.89      0.89     14400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare training and test features/labels\n",
    "X_train = np.vstack(train_df['embedding'].values)\n",
    "y_train = train_df['clean_category'].values\n",
    "\n",
    "X_test = np.vstack(test_df['embedding'].values)\n",
    "y_test = test_df['clean_category'].values\n",
    "\n",
    "# Instantiate the Logistic Regression classifier\n",
    "clf_embs = LogisticRegression(max_iter=1000, solver='liblinear', random_state=42)\n",
    "\n",
    "# Train the classifier on the labeled training data\n",
    "print(f'Train classifier on embeddings')\n",
    "clf_embs.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on the test set\n",
    "predictions = clf_embs.predict(X_test)\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train classifier on embeddings\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Bekjendtgjørelser       0.92      0.95      0.94      4800\n",
      "      Jndenlandsk       0.85      0.85      0.85      4800\n",
      "       Udenlandsk       0.90      0.87      0.88      4800\n",
      "\n",
      "         accuracy                           0.89     14400\n",
      "        macro avg       0.89      0.89      0.89     14400\n",
      "     weighted avg       0.89      0.89      0.89     14400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare training and test features/labels\n",
    "X_train = np.vstack(train_df['embedding'].values)\n",
    "y_train = train_df['clean_category'].values\n",
    "\n",
    "X_test = np.vstack(test_df['embedding'].values)\n",
    "y_test = test_df['clean_category'].values\n",
    "\n",
    "# Instantiate the Logistic Regression classifier\n",
    "clf_embs = LogisticRegression(max_iter=1000, solver='liblinear', random_state=42)\n",
    "\n",
    "# Train the classifier on the labeled training data\n",
    "print(f'Train classifier on embeddings')\n",
    "clf_embs.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on the test set\n",
    "predictions = clf_embs.predict(X_test)\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train classifier on TF-IDF features\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Bekjendtgjørelser       0.94      0.96      0.95      4800\n",
      "      Jndenlandsk       0.88      0.86      0.87      4800\n",
      "       Udenlandsk       0.90      0.91      0.91      4800\n",
      "\n",
      "         accuracy                           0.91     14400\n",
      "        macro avg       0.91      0.91      0.91     14400\n",
      "     weighted avg       0.91      0.91      0.91     14400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=5000)  # Adjust max_features as needed\n",
    "\n",
    "# Fit on training data and transform both train and test sets\n",
    "X_train = vectorizer.fit_transform(train_df['text'])\n",
    "X_test = vectorizer.transform(test_df['text'])\n",
    "\n",
    "# Prepare labels\n",
    "y_train = train_df['clean_category'].values\n",
    "y_test = test_df['clean_category'].values\n",
    "\n",
    "# Instantiate the Logistic Regression classifier\n",
    "clf_tfidf = LogisticRegression(max_iter=1000, solver='liblinear', random_state=42)\n",
    "\n",
    "# Train the classifier on the TF-IDF features\n",
    "print(f'Train classifier on TF-IDF features')\n",
    "clf_tfidf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on the test set\n",
    "predictions = clf_tfidf.predict(X_test)\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get unlabeled articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(599929, 11)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create final_df with all unlabeled articles\n",
    "dfs = []\n",
    "\n",
    "for i in range(1,5):\n",
    "    ds = Dataset.load_from_disk(f'{path_root}{newspapers[i]}')\n",
    "    df_np = ds.to_pandas()\n",
    "    dfs.append(df_np)\n",
    "\n",
    "final_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Check the shape of each embedding\n",
    "final_df['embedding_shape'] = final_df['embedding'].apply(lambda x: np.array(x).shape)\n",
    "expected_dim = final_df['embedding_shape'].max()[0]\n",
    "final_df = final_df[final_df['embedding'].apply(lambda x: np.array(x).shape == (expected_dim,))].copy()\n",
    "\n",
    "# Add column with name newspaper\n",
    "final_df['newspaper'] = final_df['article_id'].str.extract(r'^(.*?)_')\n",
    "\n",
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1449/2593171623.py:1: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_sampled = final_df.groupby('newspaper', group_keys=False).apply(lambda x: x.sample(n=200, random_state=42))\n"
     ]
    }
   ],
   "source": [
    "# Create a balanced sample for testing and manually evaluating\n",
    "df_sampled = final_df.groupby('newspaper', group_keys=False).apply(lambda x: x.sample(n=200, random_state=42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the new unlabeled data using the trained TF-IDF vectorizer\n",
    "X_test_tfidf = vectorizer.transform(df_sampled['text'])\n",
    "\n",
    "# Predict categories for the new data based on tf_idf\n",
    "df_sampled['predicted_category_tf_idf'] = clf_tfidf.predict(X_test_tfidf)\n",
    "\n",
    "X_test_embs = np.vstack(df_sampled['embedding'].values)\n",
    "\n",
    "# Predict categories for the new data based on embeddings\n",
    "df_sampled['predicted_category_embs'] = clf_embs.predict(X_test_embs)\n",
    "\n",
    "# Save results\n",
    "df_sampled[['article_id', 'date', 'predicted_category_tf_idf', 'predicted_category_embs', 'category', 'text']].to_csv('../results/predicted_sample_tfidf_embeddings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disagree = df_sampled[df_sampled['predicted_category_embs'] != df_sampled['predicted_category_tf_idf']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(117, 14)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_disagree.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: using the best classifier to predict all unlabeled articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(685021, 12)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create final_df with all unlabeled articles\n",
    "dfs = []\n",
    "\n",
    "for i in range(0,5):\n",
    "    ds = Dataset.load_from_disk(f'{path_root}{newspapers[i]}')\n",
    "    df_np = ds.to_pandas()\n",
    "    dfs.append(df_np)\n",
    "\n",
    "final_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Check the shape of each embedding\n",
    "final_df['embedding_shape'] = final_df['embedding'].apply(lambda x: np.array(x).shape)\n",
    "expected_dim = final_df['embedding_shape'].max()[0]\n",
    "final_df = final_df[final_df['embedding'].apply(lambda x: np.array(x).shape == (expected_dim,))].copy()\n",
    "\n",
    "# Add column with name newspaper\n",
    "final_df['newspaper'] = final_df['article_id'].str.extract(r'^(.*?)_')\n",
    "\n",
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "clean_category\n",
       "-1                   599929\n",
       "Bekjendtgjørelser     35817\n",
       "Jndenlandsk           25184\n",
       "Udenlandsk            24091\n",
       "Name: clean_category, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.groupby('clean_category')['clean_category'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>date</th>\n",
       "      <th>predicted_category_tf_idf</th>\n",
       "      <th>evaluation_tf_idf</th>\n",
       "      <th>predicted_category_embs</th>\n",
       "      <th>evaluation_embs</th>\n",
       "      <th>category</th>\n",
       "      <th>true_label</th>\n",
       "      <th>bog_teater</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>55097</th>\n",
       "      <td>aal_055098</td>\n",
       "      <td>1826-08-17</td>\n",
       "      <td>Jndenlandsk</td>\n",
       "      <td>t</td>\n",
       "      <td>Jndenlandsk</td>\n",
       "      <td>t</td>\n",
       "      <td>Kjøbenhavn.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>J Nærheden heraf er opkommet en Jordbrand, som...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142457</th>\n",
       "      <td>aal_142458</td>\n",
       "      <td>1840-12-05</td>\n",
       "      <td>Jndenlandsk</td>\n",
       "      <td>t</td>\n",
       "      <td>Bekjendtgjørelser</td>\n",
       "      <td>t</td>\n",
       "      <td>Bekjendtgjørelser</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ved Auctionen, Torsdagen den 10de dennes, i St...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70491</th>\n",
       "      <td>aal_070492</td>\n",
       "      <td>1829-03-04</td>\n",
       "      <td>Udenlandsk</td>\n",
       "      <td>t</td>\n",
       "      <td>Udenlandsk</td>\n",
       "      <td>t</td>\n",
       "      <td>Blandinger</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>En Kok i Paris har taget Livet af sig, fordi m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146810</th>\n",
       "      <td>aal_146811</td>\n",
       "      <td>1841-08-10</td>\n",
       "      <td>Udenlandsk</td>\n",
       "      <td>t</td>\n",
       "      <td>Udenlandsk</td>\n",
       "      <td>t</td>\n",
       "      <td>Nyeste Post-Efterretninger</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Det hollandske Blad, Tolk der Vryheid, med hvi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155768</th>\n",
       "      <td>aal_155769</td>\n",
       "      <td>1842-12-07</td>\n",
       "      <td>Jndenlandsk</td>\n",
       "      <td>t</td>\n",
       "      <td>Jndenlandsk</td>\n",
       "      <td>t</td>\n",
       "      <td>Fædrelandet</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>J Sagen: Postholder Prahl, for, imod ProviHarm...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        article_id        date predicted_category_tf_idf evaluation_tf_idf  \\\n",
       "55097   aal_055098  1826-08-17               Jndenlandsk                 t   \n",
       "142457  aal_142458  1840-12-05               Jndenlandsk                 t   \n",
       "70491   aal_070492  1829-03-04                Udenlandsk                 t   \n",
       "146810  aal_146811  1841-08-10                Udenlandsk                 t   \n",
       "155768  aal_155769  1842-12-07               Jndenlandsk                 t   \n",
       "\n",
       "       predicted_category_embs evaluation_embs                    category  \\\n",
       "55097              Jndenlandsk               t                 Kjøbenhavn.   \n",
       "142457       Bekjendtgjørelser               t           Bekjendtgjørelser   \n",
       "70491               Udenlandsk               t                  Blandinger   \n",
       "146810              Udenlandsk               t  Nyeste Post-Efterretninger   \n",
       "155768             Jndenlandsk               t                 Fædrelandet   \n",
       "\n",
       "       true_label bog_teater  \\\n",
       "55097         NaN        NaN   \n",
       "142457        NaN        NaN   \n",
       "70491         NaN        NaN   \n",
       "146810        NaN        NaN   \n",
       "155768        NaN        NaN   \n",
       "\n",
       "                                                     text  \n",
       "55097   J Nærheden heraf er opkommet en Jordbrand, som...  \n",
       "142457  Ved Auctionen, Torsdagen den 10de dennes, i St...  \n",
       "70491   En Kok i Paris har taget Livet af sig, fordi m...  \n",
       "146810  Det hollandske Blad, Tolk der Vryheid, med hvi...  \n",
       "155768  J Sagen: Postholder Prahl, for, imod ProviHarm...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold_set = pd.read_csv('../results/predicted_sample_tfidf_embeddings(1).csv', index_col=0, sep=';')\n",
    "gold_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2376/3149177884.py:1: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  gold_set = gold_set.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n"
     ]
    }
   ],
   "source": [
    "gold_set = gold_set.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "\n",
    "gold_set[\"true_label\"] = gold_set.apply(\n",
    "    lambda row: row[\"predicted_category_tf_idf\"] if row[\"evaluation_tf_idf\"] == \"t\" else\n",
    "                (row[\"predicted_category_embs\"] if row[\"evaluation_embs\"] == \"t\" else row[\"true_label\"]),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge final_df with df_gold_set on 'article_id' to get updated values\n",
    "final_df = final_df.merge(gold_set[['article_id', 'true_label']], on='article_id', how='left')\n",
    "\n",
    "# Overwrite 'clean_category' with 'true_label' where available\n",
    "final_df['clean_category'] = final_df['true_label'].fillna(final_df['clean_category'])\n",
    "\n",
    "# Drop the temporary 'true_label' column\n",
    "final_df.drop(columns=['true_label'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['label_type'] = final_df['clean_category'].apply(lambda x: 'predicted' if x == -1 else 'gold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85882, 13)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df = final_df[final_df['label_type'] == 'gold']\n",
    "training_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2376/1950500359.py:5: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  lol_balanced = lol_df.groupby('clean_category', group_keys=False).apply(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1990, 13)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify the subset where 'newspaper' == 'lol'\n",
    "lol_df = training_df[training_df['newspaper'] == 'lol']\n",
    "\n",
    "# Sample 200 random examples for each value of 'clean_category'\n",
    "lol_balanced = lol_df.groupby('clean_category', group_keys=False).apply(\n",
    "    lambda x: x.sample(n=400, replace=True) if len(x) >= 400 else x\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# Keep the other newspapers as they are\n",
    "other_newspapers_df = training_df[training_df['newspaper'] != 'lol']\n",
    "\n",
    "# Combine the balanced 'lol' subset with the other newspapers\n",
    "filtered_df = pd.concat([lol_balanced, other_newspapers_df], ignore_index=True)\n",
    "filtered_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train final classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the balanced data into train and test sets with stratification\n",
    "train_df, test_df = train_test_split(\n",
    "    filtered_df, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=filtered_df['clean_category']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train classifier on embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ucloud/.local/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1288: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Bekjendtgjørelser       0.92      0.88      0.89       136\n",
      "      Jndenlandsk       0.75      0.82      0.78       136\n",
      "       Udenlandsk       0.87      0.82      0.84       126\n",
      "\n",
      "         accuracy                           0.84       398\n",
      "        macro avg       0.84      0.84      0.84       398\n",
      "     weighted avg       0.84      0.84      0.84       398\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare training and test features/labels\n",
    "X_train = np.vstack(train_df['embedding'].values)\n",
    "y_train = train_df['clean_category'].values\n",
    "\n",
    "X_test = np.vstack(test_df['embedding'].values)\n",
    "y_test = test_df['clean_category'].values\n",
    "\n",
    "# Instantiate the Logistic Regression classifier\n",
    "clf_embs = LogisticRegression(max_iter=1000, solver='liblinear', random_state=42)\n",
    "\n",
    "# Train the classifier on the labeled training data\n",
    "print(f'Train classifier on embeddings')\n",
    "clf_embs.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on the test set\n",
    "predictions = clf_embs.predict(X_test)\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(599139, 13)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df = final_df[final_df['label_type'] == 'predicted']\n",
    "pred_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_269/2012251868.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  pred_df['predicted_category_embs'] = clf_embs.predict(X_test_embs)\n"
     ]
    }
   ],
   "source": [
    "X_test_embs = np.vstack(pred_df['embedding'].values)\n",
    "\n",
    "# Predict categories for the unlabeled articles based on embeddings\n",
    "pred_df['predicted_category_embs'] = clf_embs.predict(X_test_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge final_df with pred_df on 'article_id' to get updated values\n",
    "final_df = final_df.merge(pred_df[['article_id', 'predicted_category_embs']], on='article_id', how='left')\n",
    "\n",
    "# Overwrite 'clean_category' with 'predicted_category_embs' where available\n",
    "final_df['clean_category'] = final_df['predicted_category_embs'].fillna(final_df['clean_category'])\n",
    "\n",
    "# Drop the temporary 'true_label' column\n",
    "final_df.drop(columns=['predicted_category_embs'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(685021, 13)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Aarhus articles / Add second batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(372227, 10)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create final_df with all unlabeled articles\n",
    "dfs = []\n",
    "\n",
    "for i in range(0,3):\n",
    "    ds = Dataset.load_from_disk(f'{path_root}{newspaper_extra[i]}')\n",
    "    df_np = ds.to_pandas()\n",
    "    dfs.append(df_np)\n",
    "\n",
    "aar_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Check the shape of each embedding\n",
    "aar_df['embedding_shape'] = aar_df['embedding'].apply(lambda x: np.array(x).shape)\n",
    "expected_dim = aar_df['embedding_shape'].max()[0]\n",
    "aar_df = aar_df[aar_df['embedding'].apply(lambda x: np.array(x).shape == (expected_dim,))].copy()\n",
    "\n",
    "# Add column with name newspaper\n",
    "aar_df['newspaper'] = aar_df['article_id'].str.extract(r'^(.*?)_')\n",
    "aar_df = aar_df.drop(columns=['clean_category_y']).rename(columns={'clean_category_x': 'clean_category'})\n",
    "aar_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>date</th>\n",
       "      <th>embedding</th>\n",
       "      <th>n_chunks_orig</th>\n",
       "      <th>clean_category</th>\n",
       "      <th>text</th>\n",
       "      <th>article_length</th>\n",
       "      <th>characters</th>\n",
       "      <th>embedding_shape</th>\n",
       "      <th>newspaper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aar_182201</td>\n",
       "      <td>1830-01-01</td>\n",
       "      <td>[0.02404645597562194, -0.008350105490535498, -...</td>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>Nytaarsmorgen Et Aar er flygtet hen med Lynets...</td>\n",
       "      <td>196</td>\n",
       "      <td>1133</td>\n",
       "      <td>(1024,)</td>\n",
       "      <td>aar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aar_182202</td>\n",
       "      <td>1830-01-01</td>\n",
       "      <td>[0.02409675158560276, 0.012717369943857193, -0...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>Nyheder fra Udlandet. Tyrkiet. General Diebits...</td>\n",
       "      <td>18</td>\n",
       "      <td>121</td>\n",
       "      <td>(1024,)</td>\n",
       "      <td>aar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aar_182203</td>\n",
       "      <td>1830-01-01</td>\n",
       "      <td>[0.03033153402308623, 0.021544675730789702, -0...</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>De russiske Lazarether skulle forblive saalæng...</td>\n",
       "      <td>152</td>\n",
       "      <td>951</td>\n",
       "      <td>(1024,)</td>\n",
       "      <td>aar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aar_182204</td>\n",
       "      <td>1830-01-01</td>\n",
       "      <td>[0.032663642428815365, 0.01636680751107633, -0...</td>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>En russisk Officeer har blandt Andet skrevet h...</td>\n",
       "      <td>275</td>\n",
       "      <td>1764</td>\n",
       "      <td>(1024,)</td>\n",
       "      <td>aar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aar_182205</td>\n",
       "      <td>1830-01-01</td>\n",
       "      <td>[0.03961804881691933, 0.032092029228806496, -0...</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>Portugal. Privatbreve fra Lissabon fortælle, a...</td>\n",
       "      <td>118</td>\n",
       "      <td>742</td>\n",
       "      <td>(1024,)</td>\n",
       "      <td>aar</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id        date                                          embedding  \\\n",
       "0  aar_182201  1830-01-01  [0.02404645597562194, -0.008350105490535498, -...   \n",
       "1  aar_182202  1830-01-01  [0.02409675158560276, 0.012717369943857193, -0...   \n",
       "2  aar_182203  1830-01-01  [0.03033153402308623, 0.021544675730789702, -0...   \n",
       "3  aar_182204  1830-01-01  [0.032663642428815365, 0.01636680751107633, -0...   \n",
       "4  aar_182205  1830-01-01  [0.03961804881691933, 0.032092029228806496, -0...   \n",
       "\n",
       "   n_chunks_orig  clean_category  \\\n",
       "0              4              -1   \n",
       "1              1              -1   \n",
       "2              3              -1   \n",
       "3              4              -1   \n",
       "4              2              -1   \n",
       "\n",
       "                                                text  article_length  \\\n",
       "0  Nytaarsmorgen Et Aar er flygtet hen med Lynets...             196   \n",
       "1  Nyheder fra Udlandet. Tyrkiet. General Diebits...              18   \n",
       "2  De russiske Lazarether skulle forblive saalæng...             152   \n",
       "3  En russisk Officeer har blandt Andet skrevet h...             275   \n",
       "4  Portugal. Privatbreve fra Lissabon fortælle, a...             118   \n",
       "\n",
       "   characters embedding_shape newspaper  \n",
       "0        1133         (1024,)       aar  \n",
       "1         121         (1024,)       aar  \n",
       "2         951         (1024,)       aar  \n",
       "3        1764         (1024,)       aar  \n",
       "4         742         (1024,)       aar  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aar_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "clean_category\n",
       "-1    372227\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aar_df['clean_category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "aar_df['label_type'] = aar_df['clean_category'].apply(lambda x: 'predicted' if x == -1 else 'gold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(372227, 11)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_aar_df = aar_df[aar_df['label_type'] == 'predicted']\n",
    "pred_aar_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_embs = np.vstack(pred_aar_df['embedding'].values)\n",
    "\n",
    "# Predict categories for the unlabeled articles based on embeddings\n",
    "pred_aar_df['predicted_category_embs'] = clf_embs.predict(X_test_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge aar_df with pred_df on 'article_id' to get updated values\n",
    "aar_df = aar_df.merge(pred_aar_df[['article_id', 'predicted_category_embs']], on='article_id', how='left')\n",
    "\n",
    "# Overwrite 'clean_category' with 'predicted_category_embs' where available\n",
    "aar_df['clean_category'] = aar_df['predicted_category_embs'].fillna(aar_df['clean_category'])\n",
    "\n",
    "# Drop the temporary 'true_label' column\n",
    "aar_df.drop(columns=['predicted_category_embs'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(372227, 11)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aar_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "clean_category\n",
       "Bekjendtgjørelser    132226\n",
       "Udenlandsk           124022\n",
       "Jndenlandsk          115979\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aar_df['clean_category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8e7deab913542d58ddca14daa97bbb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "946112e2f64c4c1b80cfe7051240df3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0/16 [00:00<?, ?files/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb2695fce15c4c8e84fc68b8147150ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00010-of-00016.parquet:   0%|          | 0.00/380M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76154db957414944a39ce90edbbebfcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00012-of-00016.parquet:   0%|          | 0.00/395M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90368a15577d429f995d75314625c3f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00013-of-00016.parquet:   0%|          | 0.00/389M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6988a96d44b04e2d843e876073e7b768",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00014-of-00016.parquet:   0%|          | 0.00/387M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3f4b8085cc547c4b3fa0dadaaaaea78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00015-of-00016.parquet:   0%|          | 0.00/395M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "282b2293a8e64a9da2ac8cb193ca4d59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/866977 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(866977, 13)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add aar_df to loaded dataset from HF\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"chcaa/periphery-aviser-e5\", split='train')\n",
    "\n",
    "full_df = dataset.to_pandas()\n",
    "full_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['article_id', 'date', 'embedding', 'n_chunks_orig', 'clean_category',\n",
      "       'text', 'article_length', 'characters', 'embedding_shape', 'newspaper',\n",
      "       'label_type'],\n",
      "      dtype='object')\n",
      "Index(['article_id', 'date', 'embedding', 'n_chunks_orig', 'clean_category',\n",
      "       'nøgle', 'text', 'category', 'article_length', 'characters',\n",
      "       'embedding_shape', 'newspaper', 'label_type'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(aar_df.columns)\n",
    "print(full_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['article_id', 'date', 'embedding', 'n_chunks_orig', 'clean_category',\n",
       "       'text', 'article_length', 'characters', 'embedding_shape', 'newspaper',\n",
       "       'label_type'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df = full_df.drop(columns=['nøgle', 'category'])\n",
    "full_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1239204, 11)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df = pd.concat([full_df, aar_df])\n",
    "merged_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Convert DataFrame to Dataset\n",
    "dataset = Dataset.from_pandas(merged_df, preserve_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a65b318e078f483782267e8cf8dac94d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5413e69d35af470a86ad4749271f743e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/55 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "240a95a841ad469a82730f50d9ea2810",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/55 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3fad0fe78464594b394cb0c03916b6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/55 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aa87187f54943f29ad55cbee2e39cb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/55 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "925f375e22684aa3b644e94aecd4043c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/55 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffb6e07a140b42b0b4a4b3c402b053b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/55 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c07e0c4de9e2415e83b7e12bfa5f4c60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/55 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5af686c73977461fa13be057ae150716",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/55 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c119cf6eb088464989d44f556908c55b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/55 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12e6ea548d5b43868a33f68ee26d656f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/55 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d33b424b3bae4da7a6e6d7724ca47e55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/55 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "951beebc91eb4fdba5bae5bd7ed8c5c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/55 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4527716294b3434ca7cefebc8b299544",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/55 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bea6041f7a1f4faf95b8d484c5a9cff1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/55 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4c7de8481d946a8916be9d2bc44779a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/55 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "732788cf96d14aa2b98fec1f8365a7d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/55 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/chcaa/periphery-aviser-e5/commit/63d36a14956df84977f23783135603ac5f64e21f', commit_message='Upload dataset', commit_description='', oid='63d36a14956df84977f23783135603ac5f64e21f', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/chcaa/periphery-aviser-e5', endpoint='https://huggingface.co', repo_type='dataset', repo_id='chcaa/periphery-aviser-e5'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Push to Hugging Face as a dataset\n",
    "dataset.push_to_hub(\"chcaa/periphery-aviser-e5\", private=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save articles and their category as parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_parquet(\"../../../DATA/NEWSPAPERS/final_df_e5.parquet\", engine=\"pyarrow\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "HfHubHTTPError",
     "evalue": "401 Client Error: Unauthorized for url: https://huggingface.co/api/repos/create (Request ID: Root=1-67e12a5d-0752c99e1acd1f3e2dae440f;78524486-b840-42f6-9082-e49be6cfbc3d)\n\nInvalid username or password.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:409\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m409\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/requests/models.py:1024\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1023\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1024\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPError\u001b[39m: 401 Client Error: Unauthorized for url: https://huggingface.co/api/repos/create",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mHfHubHTTPError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m dataset = Dataset.from_pandas(final_df, preserve_index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Push to Hugging Face as a dataset\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpush_to_hub\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mawlassche/periphery-aviser-e5\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprivate\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/datasets/arrow_dataset.py:5533\u001b[39m, in \u001b[36mDataset.push_to_hub\u001b[39m\u001b[34m(self, repo_id, config_name, set_default, split, data_dir, commit_message, commit_description, private, token, revision, create_pr, max_shard_size, num_shards, embed_external_files)\u001b[39m\n\u001b[32m   5529\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSplit name should match \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_split_re\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m but got \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   5531\u001b[39m api = HfApi(endpoint=config.HF_ENDPOINT, token=token)\n\u001b[32m-> \u001b[39m\u001b[32m5533\u001b[39m repo_url = \u001b[43mapi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_repo\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5534\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5535\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5536\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdataset\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   5537\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprivate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprivate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5538\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   5539\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5540\u001b[39m repo_id = repo_url.repo_id\n\u001b[32m   5542\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m revision \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m revision.startswith(\u001b[33m\"\u001b[39m\u001b[33mrefs/pr/\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   5543\u001b[39m     \u001b[38;5;66;03m# We do not call create_branch for a PR reference: 400 Bad Request\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/huggingface_hub/hf_api.py:3511\u001b[39m, in \u001b[36mHfApi.create_repo\u001b[39m\u001b[34m(self, repo_id, token, private, repo_type, exist_ok, resource_group_id, space_sdk, space_hardware, space_storage, space_sleep_time, space_secrets, space_variables)\u001b[39m\n\u001b[32m   3508\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   3510\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3511\u001b[39m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3512\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m   3513\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exist_ok \u001b[38;5;129;01mand\u001b[39;00m err.response.status_code == \u001b[32m409\u001b[39m:\n\u001b[32m   3514\u001b[39m         \u001b[38;5;66;03m# Repo already exists and `exist_ok=True`\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:481\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    477\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    479\u001b[39m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[32m    480\u001b[39m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m481\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, \u001b[38;5;28mstr\u001b[39m(e), response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mHfHubHTTPError\u001b[39m: 401 Client Error: Unauthorized for url: https://huggingface.co/api/repos/create (Request ID: Root=1-67e12a5d-0752c99e1acd1f3e2dae440f;78524486-b840-42f6-9082-e49be6cfbc3d)\n\nInvalid username or password."
     ]
    }
   ],
   "source": [
    "# Convert DataFrame to Dataset\n",
    "dataset = Dataset.from_pandas(final_df, preserve_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "215c2b72ca4649fa8e4c21dae07bc585",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a3f1c601e364ca5a8d6b0bd056b2338",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c259aed665245878c17bfada758341a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/53 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "995a4e0b5c9f421297b7a35d5c9067f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/53 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a49b9be32b3c4853a834fed177f3c708",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/53 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bce65d6bc6334e8c9a08a7efcd1d9b91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/53 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6228700c34a409ca2f602adefa7734d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/53 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd6cf37d0f5242059a1a5261c4b9b372",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/53 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b5b6bcf5eb8492fb1cfee73f8273d08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/53 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f02ac291a45486d81a8052b258bfafa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/53 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2837458101d74b37a0934fe95226dee5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/53 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d256dc07adb4561b17a81195bba8f84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/53 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa6d12121b564f0b86dca2faa07a11f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/53 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b8979b540cd43f7b071fff196eae92e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/53 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a586c842ca544a8ac067f1341d1f96d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/53 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/awlassche/periphery-aviser-e5/commit/6e8115f1251e777d3d9bed3967834ec3171aed4c', commit_message='Upload dataset', commit_description='', oid='6e8115f1251e777d3d9bed3967834ec3171aed4c', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/awlassche/periphery-aviser-e5', endpoint='https://huggingface.co', repo_type='dataset', repo_id='awlassche/periphery-aviser-e5'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Push to Hugging Face as a dataset\n",
    "dataset.push_to_hub(\"chcaa/periphery-aviser-e5\", private=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
